\begin{document}

\section{Offline Topics}

\ft{Code Integrity}

\I David has looked at two products:
    \I scan-build analyzer
    \I cppcheck
\I finds things like
    \I unused variables
    \I bad free (free'ing somthing not malloc'ed)
    \I various null pointer scenarios
    \I memory leaks
\I detailed reports generated, ``explanations'' given
\I downside: most ``bugs'' actually innocuous (false positives)

\ft{REST format}

\I new HDDM (effective) mark-up language for reconstructed data
\I captures data in DANA objects, writes them out
\I can be read in and DANA objects reconstituted
\I large factor of compression over raw MC data
\bc Related issues\ec
\I raw MC event size
    \I size of raw events studied
    \I need to account for truth information separately
\I proposal for separating truth from fiction

\ft{Odds and Sods}

\I Xerces-c 3 support
\I Bourne shell environment set-up is out there now
\I BCAL smearing
\I EVIO-to-DANA conversion
\I Tracking progress
\I Kinematic fitting
\I Analysis tools
\I CCDB
\I 12 GeV Offline Software Review

\section{Data Challenge}

\ft{Organization}

\I Wiki page for collecting ideas
\I Bi-weekly data challenge meetings
\I Discussion of analysis tools at bi-weekly Physics Working Group meetings

\ft{Comments}
\I need robust SRM
\I JLab farm and on grid
\I huge sample of Pythia data
\I December or January goal for full-scale

\ft{Mini Data Challenges}

\I Purpose: develop system for automating large-scale production processing.
    \I how to generate and run thousands of jobs
    \I assess their status (before, during, and after they run)
    \I manage all output files and diagnostic data
\I With system in place, iteration easy
\I Can then refine:
    \I code correctness
    \I execution speed
    \I design and implementation of analysis system
    \I develop goals for full-scale challenge
    \I find bottle-necks at intermediate scale 

\ft{Current Configuration}

\I bggen
        \I create 400 k events per job with bggen = 20 s of data
        \I run mcsmear on hdgeant output
        \I write results to tape library
        \I about 7 hours of CPU time
        \I output file is about 14 GB (35 kB per event) 
\I hd\_root
        \I get bggen data file from library
        \I run reconstruction using hd_root
        \I write resulting REST file to tape library

\I Perl script and MySQL database: no by-hand operations
\I Output for both written to new tape volume set: /mss/halld/halld-scratch
\I Unit of challenge: 1000 jobs = 5.6 hours of data (1 s = 20 k events)
\includegraphics{Farm_usage_2012-09-10.png}

\section{Summary/Conclusions}

\ft{Summary and Conclusions}

\I Settled on a compact format for reconstructed data
\I Responding to review recommendations
\I Studying tools for QA of our code
\I Data Challenge technology developing
\I Have the attention of the Computer Center
\I And if there is time...

\section{JLab Computing Items}

\ft{GEANT4 Tutorial Workshop}

\I Held at JLab July 9-13
\I Reprise of 2006 workshop
\I Over 100 participants
\I Many hands-on sessions
\I All talks and tutorials available at http://geant4.slac.stanford.edu/JLAB2012/Agenda.html
\includegraphics{group8x10_lowres.jpg}

\ft{Scientific Computing Survey}

\bi
\I User Group Board of Directors sponsored
\I Running Period: 4 weeks, starting July 23 through August 20, 2012
\I Questions: 16 multiple choice questions, 2 essay-type questions
\I Number of responses: 118
\ei
\I Impressions from written responses}
\bi
\I standard package support desired (ROOT, GEANT) 
\I large-scale data transfer on/off-site difficult

\end{document}
