\documentclass[xcolor=dvipsnames,hyperref={pdfpagelabels=false}]{beamer}

\usetheme{Boadilla}

\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newcommand{\bd}{\begin{description}}
\newcommand{\ed}{\end{description}}
\newcommand{\I}{\item}
\newcommand{\f}{\frame}
\newcommand{\ft}{\frametitle}

\title{Offline Software Overview}
\subtitle{GlueX Collaboration Meeting}
\author[Mark Ito]{Mark M.\ Ito}
\date{May 13, 2014}
\institute[JLab]{Jefferson Lab}

\begin{document}

\f{\titlepage}

\f{\ft{Outline}
\bi
\I Data Challenge
  \bi
  \I goals
  \I conditions
  \I sites
  \I organization
  \I issues
  \I results
  \I next
  \I outcomes
  \ei
\I Other Topics
\I To Do
\ei
}

\f{\ft{Goals}
    include curtis's slide:  February  2014 Data Challenge
\bi
\I nominal goal 10 billion events.
\I Pythia/BGGEN events from 7.0GeV to the endpoint.
\I Target distribution to be nominal LH2 as handled by HDGEANT.
\I 1350 A current for magnetic field
\I updated REST format, matching info saved
\I use the CCDB
\ei
}
  
\f{\ft{Conditions}
\bi
\I software versions specified
\I background settings \\
\begin{tabular}{|c|c|c|}
\hline
Run & $\gamma$ Rate & fraction \\
\hline
9001 & $1\times 10^7$ & 80\% \\
9002 & $5\times 10^7$ & 5\% \\
9003 & 0 & 15\% \\
\hline
\end{tabular}
\I file number assignments for each site
\I configuration files specified
\I command-lines specified
\ei
}

\f{\ft{Sites}
\bi
\I Carnegie Mellon
  \bi
  \I 384 cores
  \ei
\I Florida State
  \bi
  \I 144 cores
  \ei
\I JLab
  \bi
  \I 400 to 2400 cores
  \I workflow tools at JLab not ready
  \ei
\I MIT
  \bi
  \I 300+ cores
  \I OpenStack at MIT
    \bi
    \I Virtual Machines instances
    \I MIT Re-Use Cluster
    \I FutureGrid
    \ei
  \ei
\I Open Science Grid (OSG)
  \bi
  \I up to 10,000 cores
  \I nodes contributed from UConn and Northwestern
  \ei
\ei
}

\f{\ft{Organization}
\bi
\I Curtis initiated series of weekly meetings
  \bi
  \I 10 held: January 31 through April 11
  \I in addition to Offline meetings
  \ei
\I conditions webpage
  \bi
  \I Subversion controlled
  \ei
\I Event Tally Board
  \bi
  \I Google Drive spreadsheet
  \ei
\ei
}

\f{conditions webpage}
\f{back to organization page}
\f{Event Tally Board}
\f{back to organization page}

\f{\ft{Issues}
\bi
    Reversed magnetic field fixes from Simon
    effect of EM background on file size and execution time
      Kei study, varied gate widths as well
    phi = 0 problem in CDC solved
    event geneology checked
    A fix from Simon for single-ended TOF counters.
    Improvements from Paul for cutting off processing for multi-lap curling tracks. big memory footprint effect
    ZFATAL fix from richard, showers from EM background, squelched
    short rest, compression
       short rest file fix from richard: bug in xstream library with compression enabled
    non-reproducibility
       reproducibility fixed: sort needed on associated objects in STL map
    random number seeds
      looked at scheme where every event stores its seed at each stage
      settled on bggen seed
    Kei studies on em background for pπ+π- pπ+π-π0
    job tracking with farm at JLab info
    python script to scan monitoring histograms from Sean
\ei
}

\end{document}

data challenge
  results
    code frozen on 3/20
    events produced
    execution time
      running times for various em bg conditions from Paul (3/7)
    memory usage
    reliability:
      CMU: 7000 jobs, 3 failures
      JLab: 69,000 jobs, about 30 failures
    node history
      farm nodes increase at JLab
        The current farm is at about 1200 cores. The farm typically 1400 cores. For the 25% level test we will need 1250 cores. The plan is to bring another 1000 cores over from LQCD to keep the farm generally usable.
        Lattice nodes are available to us because Physics has been lending 32 16-core nodes to the LQCD farm all of December and January.
        3400 cores now in farm, 2400 for Hall D (3/4)
        plot from Mark
        plot from Sandy
    ramp down April 11 plus or minus a few days, last data challenge meeting
  data distribution
    srm
      archiving all data to tape library
    event store
    paul: proposed skims
  next data challenge
    multi threading
    tape overhead
    needed: raw data, ability to analyze it
    time frame
  what came out
    em background
    usable data set
    large usable data set
    improvements of techniques at various sites
    collaborative efforts can succeed
    need for geant4
    monitoring system
      rootspy?
other topics
  CCDB 1.00 released
  kinematic fitter update
  paul: proposed skims
  database and webserver changes coming up
  nightly build using scons
  bluejeans deployed: Curtis
summary
  to do
    splitting up packages
    re-org of offline wiki pages

